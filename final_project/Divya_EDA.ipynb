{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836c6071-5217-4646-8b2f-0b96d44878b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../final_project/includes/includes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b826614-f46d-4d54-810b-a0fe868c1d5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the data file into a DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(BIKE_TRIP_DATA_PATH)\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d4dfda-c52d-4cb2-9e8e-6d6bffa94627",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# History Bike trip\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "bike_schema=StructType([\n",
    "StructField(\"ride_id\", StringType(), True),\n",
    "StructField(\"rideable_type\", StringType(), True),\n",
    "StructField(\"started_at\", StringType(), True),\n",
    "StructField(\"ended_at\", StringType(), True),\n",
    "StructField(\"start_station_name\", StringType(), True),\n",
    "StructField(\"start_station_id\", StringType(), True),\n",
    "StructField(\"end_station_name\", StringType(), True),\n",
    "StructField(\"end_station_id\", StringType(), True),\n",
    "StructField(\"start_lat\", StringType(), True),\n",
    "StructField(\"start_lng\", StringType(), True),\n",
    "StructField(\"end_lat\", StringType(), True),\n",
    "StructField(\"end_lng\", StringType(), True),\n",
    "StructField(\"member_casual\", StringType(), True)\n",
    "])\n",
    "\n",
    "import os\n",
    "# Read data from a CSV file in batch mode\n",
    "history_bike_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(BIKE_TRIP_DATA_PATH)\n",
    "\n",
    "# Write the processed data to a Parquet file\n",
    "output_path = GROUP_DATA_PATH + \"/bronze/history_bike_trips\"\n",
    "\n",
    "if not os.path.isdir(output_path):\n",
    "    dbutils.fs.mkdirs(output_path)\n",
    "\n",
    "history_bike_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "history_bike_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"history_bike_trips\")\n",
    "\n",
    "# verify the write\n",
    "display(history_bike_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec89e65-7582-4fe8-87b9-4f616110ddab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# History Weather trip\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "weather_schema=StructType([\n",
    "    StructField(\"dt\", StringType(), True),\n",
    "    StructField(\"temp\", StringType(), True),\n",
    "    StructField(\"feels_like\", StringType(), True),\n",
    "    StructField(\"pressure\", StringType(), True),\n",
    "    StructField(\"humidity\", StringType(), True),\n",
    "    StructField(\"dew_point\", StringType(), True),\n",
    "    StructField(\"uvi\", StringType(), True),\n",
    "    StructField(\"clouds\", StringType(), True),\n",
    "    StructField(\"visibility\", StringType(), True),\n",
    "    StructField(\"wind_speed\", StringType(), True),\n",
    "    StructField(\"wind_deg\", StringType(), True),\n",
    "    StructField(\"pop\", StringType(), True),\n",
    "    StructField(\"snow_1h\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"main\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"icon\", StringType(), True),\n",
    "    StructField(\"loc\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"lon\", StringType(), True),\n",
    "    StructField(\"timezone\", StringType(), True),\n",
    "    StructField(\"timezone_offset\", StringType(), True),\n",
    "    StructField(\"rain_1h\", StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "import os\n",
    "weather_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    ".option(\"Schema\",\"weather_schema\") \\\n",
    "    .load(NYC_WEATHER_FILE_PATH)\n",
    "\n",
    "output_path = GROUP_DATA_PATH + \"/bronze/historic_weather\"\n",
    "\n",
    "if not os.path.isdir(output_path):\n",
    "    dbutils.fs.mkdirs(output_path)\n",
    "\n",
    "weather_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "weather_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"historic_weather_info\")\n",
    "\n",
    "display(weather_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d1c248-4231-426e-8ef3-a44b73ce5eab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Trips year-wise\n",
    "\n",
    "from pyspark.sql.functions import year, month, count\n",
    "\n",
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "df = df.withColumn(\"started_at\", df[\"started_at\"].cast(\"date\"))\n",
    "\n",
    "monthly_trips = df.groupBy(year(\"started_at\").alias(\"year\"), month(\"started_at\").alias(\"month\")) \\\n",
    "                  .agg(count(\"*\").alias(\"trips\")) \\\n",
    "                  .orderBy(\"year\", \"month\")\n",
    "\n",
    "display(monthly_trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779b1068-3b50-49c5-b5b2-d4b2625eca97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Line chart for Monthly Trips Over Time\n",
    "from pyspark.sql.functions import concat, lit\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "monthly_trips = monthly_trips.withColumn(\"year_month\", \n",
    "                    concat(monthly_trips[\"year\"], lit(\"-\"), monthly_trips[\"month\"]))\n",
    "\n",
    "sns.lineplot(x=\"year_month\", y=\"trips\", data=monthly_trips.toPandas())\n",
    "\n",
    "plt.title(\"Monthly Trips Over Time\")\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7067a3a-4e02-43a7-a0a0-69f90138155e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bar Chart for Monthly Trips by Year\n",
    "\n",
    "sns.catplot(x=\"month\", y=\"trips\", hue=\"year\", kind=\"bar\", data=monthly_trips.toPandas(), height=6, aspect=2)\n",
    "\n",
    "plt.title(\"Monthly Trips by Year\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfc4510-e863-4db2-bde7-f2b4b4b3763d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Heat Map for Monthly Trips by Year\n",
    "\n",
    "pivot_table = monthly_trips.toPandas().pivot(\"month\", \"year\", \"trips\")\n",
    "\n",
    "sns.heatmap(pivot_table, cmap=\"YlGnBu\", annot=True, fmt=\".0f\")\n",
    "\n",
    "plt.title(\"Monthly Trips by Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84523453-f270-4985-b334-40537f8cf80c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "print('Number of rows: ', df.count())\n",
    "print('Number of columns: ', len(df.columns))\n",
    "\n",
    "print('Distinct rideable types', df.select('rideable_type').distinct().count())\n",
    "df.groupBy('rideable_type').count().show()\n",
    "print('Distinct end station names', df.select('end_station_name').distinct().count())\n",
    "df.groupBy('end_station_name').count().show()\n",
    "print('Distinct membership types', df.select('member_casual').distinct().count())\n",
    "df.groupBy('member_casual').count().show().asc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d074a2d8-b938-4ada-af2a-26d825fa7445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rideable_counts = df.groupBy('rideable_type').count().orderBy('count', ascending=False).toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(rideable_counts['rideable_type'], rideable_counts['count'])\n",
    "plt.title('Rideable Type Counts')\n",
    "plt.xlabel('Rideable Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6303ae02-7e6b-41bf-9353-841eec9628ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "hourly_counts = df.groupBy(hour('started_at').alias('hour')).count().orderBy('hour').toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(hourly_counts['hour'], hourly_counts['count'], marker='o')\n",
    "plt.title('Hourly Ride Counts')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Ride Count')\n",
    "plt.xticks(range(0,24))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa2b78d-35aa-404a-95eb-7a17da2f6bdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, dayofweek, count, sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivot_table = df.groupBy(date_format('started_at', 'EEEE').alias('day'), 'member_casual')\\\n",
    "                .agg(count('*').alias('ride_count'))\\\n",
    "                .groupBy('day').pivot('member_casual')\\\n",
    "                .agg(sum('ride_count'))\\\n",
    "                .orderBy(dayofweek('day'))\n",
    "\n",
    "pivot_table_pd = pivot_table.toPandas()\n",
    "\n",
    "pivot_table_pd['member'] = pivot_table_pd['member'].astype(float)\n",
    "pivot_table_pd['casual'] = pivot_table_pd['casual'].astype(float)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(pivot_table_pd['day'], pivot_table_pd['casual'], label='Casual')\n",
    "plt.bar(pivot_table_pd['day'], pivot_table_pd['member'], bottom=pivot_table_pd['casual'], label='Member')\n",
    "plt.title('Ride Counts by Day of Week and Membership Type')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Ride Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c10020d-5ade-4da1-b699-e09aa4bf0ee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ride_counts = df.groupby(['rideable_type', 'member_casual']).count().select(['rideable_type', 'member_casual', 'count']).toPandas()\n",
    "\n",
    "pivot_table = ride_counts.pivot(index='member_casual', columns='rideable_type', values='count')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivot_table.plot(kind='bar', stacked=True)\n",
    "plt.title('Rideable Type and Customer Type')\n",
    "plt.xlabel('Customer Type')\n",
    "plt.ylabel('Number of Rides')\n",
    "plt.legend(title='Rideable Type', loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3323cda0-4ce4-4e1a-b90a-50b3d43e0c11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Daily Trip Trends\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "\n",
    "df_q2 = spark.read.format(\"delta\").load(delta_path)\n",
    "df_q2 = df_q2.withColumn(\"start_date\", date_format(\"started_at\", \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "daily_trips = df_q2.groupBy(\"start_date\").count()\n",
    "daily_trips = daily_trips.orderBy(\"start_date\")\n",
    "\n",
    "display(daily_trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bbc709f-1953-4567-a4ce-222f49e68c49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# line chart for daily bike trip trends\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(daily_trips.toPandas(), x=\"start_date\", y=\"count\", title=\"Daily Bike Trip Trends\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a1dde1-3b52-487c-818e-ed8621eb7024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(x=daily_trips.toPandas()['start_date'], y=daily_trips.toPandas()['count'], \n",
    "                             marker=dict(color='blue'))])\n",
    "fig.update_layout(title='Total Trips by Day', xaxis_title='Day', yaxis_title='Trips')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df2ad2b-6e7e-43ca-885a-1d0c8350c107",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b0eb27-a619-40d8-a067-a5e5e50a6a15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "from pyspark.sql.functions import date_format\n",
    "import holidays\n",
    "us_holidays_2020 = holidays.US(years=2020)\n",
    "holidays = [str(date) for date in us_holidays_2020.keys()]\n",
    "# Load the Delta table as a DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "df = df.withColumn(\"start_date\", date_format(\"started_at\", \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#holidays = [\"2020-01-01\", \"2020-01-20\", \"2020-02-17\", \"2020-05-25\", \"2020-07-03\", \"2020-09-07\", \"2020-11-26\", \"2020-12-25\"]\n",
    "\n",
    "df = df.withColumn(\"is_holiday\", F.when(F.col(\"started_at\").isin(holidays), 1).otherwise(0))\n",
    "daily_trips = df.groupBy(\"started_at\", \"is_holiday\").agg(F.count(\"*\").alias(\"trips\"))\n",
    "non_holiday_trips = daily_trips.filter(F.col(\"is_holiday\") == 0).groupBy(\"started_at\").agg(F.sum(\"trips\").alias(\"trips\"))\n",
    "weekly_trips = non_holiday_trips.groupBy(F.weekofyear(\"started_at\").alias(\"week\")).agg(F.sum(\"trips\").alias(\"trips\")).orderBy(\"week\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "sns.lineplot(x=\"week\", y=\"trips\", data=weekly_trips.toPandas())\n",
    "\n",
    "plt.title(\"Weekly Trips Over Time (Excluding Holidays)\")\n",
    "plt.xlabel(\"Week of Year\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f442cf81-033d-4d9b-a2af-d9f55f6a0511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "from pyspark.sql.functions import date_format\n",
    "import holidays\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Define US holidays\n",
    "us_holidays_2020 = holidays.US(years=2020)\n",
    "holidays = [str(date) for date in us_holidays_2020.keys()]\n",
    "\n",
    "# Load the Delta table as a DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "df = df.withColumn(\"start_date\", date_format(\"started_at\", \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "# Filter out weekends and holidays\n",
    "df = df.withColumn(\"day_of_week\", F.date_format(F.col(\"start_date\"), \"E\"))\n",
    "df = df.withColumn(\"is_weekday\", F.when(F.col(\"day_of_week\").isin([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]), 1).otherwise(0))\n",
    "df = df.withColumn(\"is_holiday\", F.when(F.col(\"start_date\").isin(holidays), 1).otherwise(0))\n",
    "df = df.filter((F.col(\"is_weekday\") == 1) & (F.col(\"is_holiday\") == 0))\n",
    "\n",
    "daily_trips = df.groupBy(\"start_date\").agg(F.count(\"*\").alias(\"trips\"))\n",
    "weekly_trips = daily_trips.groupBy(F.weekofyear(\"start_date\").alias(\"week\")).agg(F.sum(\"trips\").alias(\"trips\")).orderBy(\"week\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "sns.lineplot(x=\"week\", y=\"trips\", data=weekly_trips.toPandas())\n",
    "\n",
    "plt.title(\"Weekly Trips Over Time (Excluding Weekends and Holidays)\")\n",
    "plt.xlabel(\"Week of Year\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f76c3c-5990-471a-8ba4-f6f0e783e91c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Divya_EDA",
   "notebookOrigID": 239457942807363,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
